{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UIyuR2-QchH",
        "outputId": "adf6279c-3cf7-48d9-bab7-3586b9cc4aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text: Hello, how are you?\n",
            "Translated text: Hola\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import requests\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    filtered_words = [word for word in words if word.isalnum() and word not in stopwords.words('english')]\n",
        "\n",
        "    return filtered_words\n",
        "\n",
        "# Function to translate text using Google Translate API\n",
        "def translate_text(text, source_lang, target_lang):\n",
        "    # Google Translate API endpoint\n",
        "    url = 'https://translate.googleapis.com/translate_a/single'\n",
        "\n",
        "    # Parameters for the API request\n",
        "    params = {\n",
        "        'client': 'gtx',\n",
        "        'sl': source_lang,\n",
        "        'tl': target_lang,\n",
        "        'dt': 't',\n",
        "        'q': text\n",
        "    }\n",
        "\n",
        "    # Send GET request to Google Translate API\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    # Extract translated text from the response\n",
        "    translated_text = response.json()[0][0][0]\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "# Example usage\n",
        "input_text = \"Hello, how are you?\"\n",
        "source_language = 'en'\n",
        "target_language = 'es'  # Spanish\n",
        "\n",
        "# Preprocess the input text\n",
        "preprocessed_text = preprocess_text(input_text)\n",
        "\n",
        "# Translate the preprocessed text\n",
        "translated_text = translate_text(' '.join(preprocessed_text), source_language, target_language)\n",
        "\n",
        "print(f\"Input text: {input_text}\")\n",
        "print(f\"Translated text: {translated_text}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download NLTK resources if needed\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = tag[0].upper()\n",
        "    tag_dict = {\n",
        "        'J': wordnet.ADJ,\n",
        "        'N': wordnet.NOUN,\n",
        "        'V': wordnet.VERB,\n",
        "        'R': wordnet.ADV\n",
        "    }\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Tokenize and POS tag the text\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "    tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]\n",
        "    return tagged_sentences\n",
        "\n",
        "def grammar_check(tagged_sentences):\n",
        "    \"\"\"Perform grammar checking\"\"\"\n",
        "    errors = []\n",
        "    for tagged_sentence in tagged_sentences:\n",
        "        for word, pos in tagged_sentence:\n",
        "            wordnet_pos = get_wordnet_pos(pos)\n",
        "            if not wordnet.synsets(word, wordnet_pos):\n",
        "                errors.append((word, pos))\n",
        "    return errors\n",
        "\n",
        "# Example usage\n",
        "input_text = \"He plays soccer good.\"\n",
        "tagged_text = preprocess_text(input_text)\n",
        "errors = grammar_check(tagged_text)\n",
        "\n",
        "if errors:\n",
        "    print(\"Grammar Errors:\")\n",
        "    for word, pos in errors:\n",
        "        print(f\"Word: {word}, POS: {pos}\")\n",
        "else:\n",
        "    print(\"No grammar errors found.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYlKaMVqQnEG",
        "outputId": "60c66070-9dc1-4826-9394-a0334ad062ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar Errors:\n",
            "Word: soccer, POS: RB\n",
            "Word: ., POS: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.chat.util import Chat, reflections\n",
        "\n",
        "# Define expert system rules\n",
        "expert_system_rules = [\n",
        "    (r'What is your name?', ['I am an expert system chatbot.']),\n",
        "    (r'What can you do\\??', ['I can help you with expert system related queries.']),\n",
        "    (r'How can I (.*)', ['What would you like to know about expert systems?']),\n",
        "    # Add more rules as needed\n",
        "]\n",
        "\n",
        "def expert_system_chatbot():\n",
        "    print(\"Welcome to the Expert System Chatbot. Type 'quit' to exit.\")\n",
        "    chatbot = Chat(expert_system_rules, reflections)\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        else:\n",
        "            response = chatbot.respond(user_input)\n",
        "            print(\"Chatbot:\", response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    expert_system_chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSGbIGRxS-0h",
        "outputId": "b7484649-76a8-4190-ac6f-019bfa502499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Expert System Chatbot. Type 'quit' to exit.\n",
            "You: What is your name?\n",
            "Chatbot: I am an expert system chatbot.\n",
            "You: What can you do?\n",
            "Chatbot: I can help you with expert system related queries.\n",
            "You: quit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "orX5qTqURyOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lknoY0MYRyTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}